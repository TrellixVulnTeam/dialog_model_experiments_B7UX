{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('/data/users/kyle.shaffer/dialog_data/cornell_movie_dialog_no_context_valid_retok.txt', sep='\\t',\n",
    "                      names=['left', 'right', 'conv_id'])\n",
    "print(valid_df.shape)\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(input_df):\n",
    "    all_convos = []\n",
    "    for grp_ix, grp in input_df.groupby('conv_id'):\n",
    "        left, right = grp.left.tolist(), grp.right.tolist()\n",
    "        unrolled_convo = []\n",
    "        for lzip, rzip in zip(left, right):\n",
    "            if not(lzip in unrolled_convo):\n",
    "                unrolled_convo.append(lzip)\n",
    "            if not(rzip in unrolled_convo):\n",
    "                unrolled_convo.append(rzip)\n",
    "                \n",
    "        all_convos.append(unrolled_convo)\n",
    "            \n",
    "    return all_convos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_convos = get_contexts(valid_df)\n",
    "print(len(all_convos))\n",
    "all_convos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_convos[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def batch_convos(convos, n=3):\n",
    "    batched_convos = []\n",
    "    pad = '<PAD>'\n",
    "    for c in tqdm(convos):\n",
    "        if len(c) < n:\n",
    "            diff = n - len(c)\n",
    "            max_len = max([len(u.split()) for u in c])\n",
    "            padded_sent = [pad] * max_len\n",
    "            for _ in range(diff):\n",
    "                c.insert(0, padded_sent)\n",
    "            batched_convos.append(c)\n",
    "        elif len(c) == 3:\n",
    "            batched_convos.append(c)\n",
    "        else:\n",
    "            for u_ix in range(len(c) - (n - 1)):\n",
    "                batched_convos.append(c[u_ix: (u_ix + n)])\n",
    "                \n",
    "    return batched_convos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_convos = batch_convos(all_convos)\n",
    "len(batched_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_convos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_convos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_convos[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_convos[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_convos[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_convos[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_convos[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_convos[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/users/kyle.shaffer/dialog_data/cornell_movie_context_train.txt', mode='w') as outfile:\n",
    "    for bc in batched_convos:\n",
    "        if isinstance(bc[0], str):\n",
    "            outfile.write(bc[0])\n",
    "        else:\n",
    "            outfile.write(' '.join(bc[0]))\n",
    "        outfile.write('\\t')\n",
    "        outfile.write(bc[1])\n",
    "        outfile.write('\\t')\n",
    "        outfile.write(bc[2])\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-checking and writing out new vocab\n",
    "from collections import Counter\n",
    "\n",
    "# vocab_cnt = Counter()\n",
    "\n",
    "for l, r in zip(valid_df.left.tolist(), valid_df.right.tolist()):\n",
    "    vocab_cnt.update(l.split())\n",
    "    vocab_cnt.update(r.split())\n",
    "    \n",
    "print('Vocab Size:', len(vocab_cnt))\n",
    "vocab_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/users/kyle.shaffer/dialog_data/cornell_movie_vocab.txt', mode='w') as outfile:\n",
    "    for w, c in vocab_cnt.most_common():\n",
    "        outfile.write(w)\n",
    "        outfile.write('\\t')\n",
    "        outfile.write(str(c))\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_movie_file = '/data/users/kyle.shaffer/dialog_data/cornell_movie/dialogs_text.txt'\n",
    "data_path = \"/data/users/kyle.shaffer/dialog_data/cornell_movie/cornell_movie_dialog_no_context_train.txt\"\n",
    "\n",
    "def load_movie_text(input_file:str):\n",
    "    movie_lines = []\n",
    "    with open(input_file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            movie_lines.append(line.strip())\n",
    "\n",
    "    return movie_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304444\n"
     ]
    }
   ],
   "source": [
    "movie_lines = load_movie_text(default_movie_file)\n",
    "print(len(movie_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tgt_vocab_size = 15000\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "                            movie_lines, target_vocab_size=tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered vocab size: 14912\n"
     ]
    }
   ],
   "source": [
    "print('Discovered vocab size:', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    def __init__(self, max_len:int, tokenizer, train_file:str, valid_file:str, batch_size:int):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_file = train_file\n",
    "        self.valid_file = valid_file\n",
    "        self.bos = self.tokenizer.vocab_size\n",
    "        self.eos = self.tokenizer.vocab_size + 1\n",
    "        self.vocab_size = self.tokenizer.vocab_size + 2\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def pad_batch(self, encoder_batch, decoder_batch):\n",
    "        max_enc_length = self.max_len # max([len(s) for s in encoder_batch])\n",
    "        max_dec_length = self.max_len # max([len(s) for s in decoder_batch])\n",
    "\n",
    "        if max_enc_length > self.max_len:\n",
    "            max_enc_length = self.max_len\n",
    "        if max_dec_length > self.max_len:\n",
    "            meax_dec_length = self.max_len\n",
    "\n",
    "        enc_container, dec_in_container, dec_out_container = [], [], []\n",
    "        for enc_seq, dec_seq in zip(encoder_batch, decoder_batch):\n",
    "            if len(enc_seq) >= max_enc_length:\n",
    "                enc_seq = enc_seq[:max_enc_length]\n",
    "            enc_seq.insert(0, self.bos)\n",
    "            enc_seq.append(self.eos)\n",
    "            enc_container.append(enc_seq)\n",
    "\n",
    "            if len(dec_seq) >= max_dec_length:\n",
    "                dec_seq = dec_seq[:max_dec_length]\n",
    "            dec_out_seq = dec_seq[:]\n",
    "            dec_out_seq.append(self.eos)\n",
    "            dec_seq.insert(0, self.bos)\n",
    "\n",
    "            dec_in_container.append(dec_seq)\n",
    "            dec_out_container.append(dec_out_seq)\n",
    "\n",
    "        enc_padded = tf.keras.preprocessing.sequence.pad_sequences(enc_container, padding='post', maxlen=self.max_len)\n",
    "        dec_in_padded = tf.keras.preprocessing.sequence.pad_sequences(dec_in_container, padding='post', maxlen=self.max_len)\n",
    "        dec_out_padded = tf.keras.preprocessing.sequence.pad_sequences(dec_out_container, padding='post', maxlen=self.max_len)\n",
    "\n",
    "        return enc_padded, dec_in_padded, dec_out_padded\n",
    "\n",
    "    def get_line(self, data_file):\n",
    "        with open(data_file, mode='r') as infile:\n",
    "            for line in infile:\n",
    "                context, response, _ = line.strip().split('\\t')\n",
    "                # context_bpe, response_bpe = self.tokenizer.encode(context), self.tokenizer.encode(response)\n",
    "                # yield context_bpe, response_bpe\n",
    "                if len(context.strip().split()) > self.max_len:\n",
    "                    context = context.strip().split()[:self.max_len]\n",
    "                if len(response.strip().split()) > self.max_len:\n",
    "                    response = response.strip().split()[:self.max_len]\n",
    "                yield context, response\n",
    "\n",
    "    def batch_generator(self, mode:str='train'):\n",
    "        assert mode in {'train', 'valid'}, \"Please select as valid mode from: {train, valid}!\"\n",
    "        data_file = self.train_file if mode == 'train' else self.valid_file\n",
    "        \n",
    "        while True:\n",
    "            encoder_batch, decoder_batch = [], []\n",
    "            for context, resp in self.get_line(data_file):\n",
    "                encoder_batch.append(context)\n",
    "                decoder_batch.append(resp)\n",
    "                if len(encoder_batch) == self.batch_size:\n",
    "                    enc_padded, dec_in_padded, dec_out_padded = self.pad_batch(encoder_batch, decoder_batch)\n",
    "                    yield [enc_padded, dec_in_padded], dec_out_padded\n",
    "\n",
    "                    encoder_batch, decoder_batch = [], []\n",
    "\n",
    "            # Check for non-empty batches\n",
    "            if len(encoder_batch) > 0:\n",
    "                enc_padded, dec_in_padded, dec_out_padded = self.pad_batch(encoder_batch, decoder_batch)\n",
    "                yield [enc_padded, dec_in_padded], dec_out_padded\n",
    "                \n",
    "    def batch_gen_test(self, mode:str='train'):\n",
    "        data_file = self.train_file if mode == 'train' else self.valid_file\n",
    "        \n",
    "        while True:\n",
    "            encoder_batch, decoder_batch = [], []\n",
    "            for context, resp in self.get_line(data_file):\n",
    "                encoder_batch.append(context)\n",
    "                decoder_batch.append(resp)\n",
    "                if len(encoder_batch) == self.batch_size:\n",
    "                    yield encoder_batch, decoder_batch\n",
    "                    encoder_batch, decoder_batch = [], []\n",
    "\n",
    "            # Check for non-empty batches\n",
    "            if len(encoder_batch) > 0:\n",
    "                yield [encoder_batch, decoder_batch], dec_out_padded\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataProcessor(max_len=100, tokenizer=tokenizer, train_file=data_path,\n",
    "                                valid_file=data_path, batch_size=10)\n",
    "train_datagen = data_processor.batch_generator(mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    c, r = next(data_processor.get_line(data_path))\n",
    "    print(c)\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f174f3f1f0e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datagen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-2ddc5e0f1653>\u001b[0m in \u001b[0;36mbatch_generator\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mdecoder_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0menc_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_in_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_out_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menc_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_in_padded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_out_padded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2ddc5e0f1653>\u001b[0m in \u001b[0;36mpad_batch\u001b[0;34m(self, encoder_batch, decoder_batch)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_enc_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0menc_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_enc_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0menc_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0menc_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0menc_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "x, y = next(train_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
