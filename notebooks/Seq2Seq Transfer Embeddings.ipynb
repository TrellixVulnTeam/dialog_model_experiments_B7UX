{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vocab_path = '/data/users/kyle.shaffer/dialog_data/cornell_movie_vocab.txt'\n",
    "polar_vocab_path = '/data/users/kyle.shaffer/dialog_data/polar_vocab.txt'\n",
    "\n",
    "movie_vocab = pd.read_csv(movie_vocab_path, sep='\\t', encoding='utf8', names=['word', 'w_count', 'dialog_id'])\n",
    "\n",
    "polar_voc = []\n",
    "\n",
    "with open(polar_vocab_path, mode='r') as infile:\n",
    "    for line in infile:\n",
    "        w, w_c = line.strip().split('\\t')\n",
    "        polar_voc.append([w, int(w_c)])\n",
    "        \n",
    "polar_vocab = pd.DataFrame(polar_voc)\n",
    "del polar_voc\n",
    "polar_vocab.columns = ['word', 'w_count']\n",
    "polar_vocab = polar_vocab[polar_vocab.w_count >= 10]\n",
    "polar_vocab.head(10)\n",
    "\n",
    "print(movie_vocab.shape)\n",
    "print(polar_vocab.shape)\n",
    "\n",
    "print('Number of overlapping words:', len(set(movie_vocab.word.values) & set(polar_vocab.word.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vocab[movie_vocab.w_count >= 10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlap_words = [w for w in movie_vocab.word.values if w not in set(polar_vocab.word.values)]\n",
    "print(len(non_overlap_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_movie_word_df = movie_vocab[movie_vocab.word.isin(set(non_overlap_words))]\n",
    "print(new_movie_word_df.shape)\n",
    "new_movie_word_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_movie_word_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'This'.isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '---a'\n",
    "not(all(c in set(string.punctuation) for c in s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def filter_vocab(input_vocab):\n",
    "    num_not_string = len([w for w in input_vocab if not(isinstance(w, str))])\n",
    "    assert num_not_string == 0, '{} non-string items found...'.format(num_not_string)\n",
    "    \n",
    "    punct = set(string.punctuation)\n",
    "    cut_words = {'<SOD>', '<EOD>', '``', \"''\", 'alL'}\n",
    "    \n",
    "    new_voc = [w for w in input_vocab if not(w.isupper())]\n",
    "    new_voc = [w for w in new_voc if not w in cut_words]\n",
    "    new_voc = [w for w in new_voc if not(all(c in punct for c in w))]\n",
    "    \n",
    "    return new_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_movie_word_df.shape)\n",
    "new_movie_word_df = new_movie_word_df[new_movie_word_df.word.notnull()]\n",
    "print('New shape:', new_movie_word_df.shape)\n",
    "new_movie_word_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_movie_words = filter_vocab(new_movie_word_df.word.tolist())\n",
    "print(len(new_movie_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vocab_filter = new_movie_word_df[new_movie_word_df.word.isin(set(new_movie_words))]\n",
    "print(movie_vocab_filter.shape)\n",
    "movie_vocab_filter.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Munging Work: Need to Untokenize and Retokenize Movie Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tok = TweetTokenizer()\n",
    "tok.tokenize(\"I'm sorry to hear about your friends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacremoses import MosesDetokenizer\n",
    "\n",
    "detok = MosesDetokenizer(lang='en')\n",
    "detok.detokenize(\"I 'm sorry to hear about your friends .\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_train_path = '/data/users/kyle.shaffer/dialog_data/cornell_movie_dialog_no_context_train.txt'\n",
    "cornell_valid_path = '/data/users/kyle.shaffer/dialog_data/cornell_movie_dialog_no_context_valid.txt'\n",
    "\n",
    "cornell_train_file = open(cornell_train_path, mode='r')\n",
    "cornell_valid_file = open(cornell_valid_path, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open('/data/users/kyle.shaffer/dialog_data/cornell_movie_dialog_no_context_valid_retok.txt', mode='w') as outfile:\n",
    "    for line in tqdm(cornell_valid_file):\n",
    "        left, right, conv_id = line.strip().split('\\t')\n",
    "        left_string = detok.detokenize(left.strip().split())\n",
    "        right_string = detok.detokenize(right.strip().split())\n",
    "        \n",
    "        left_new = ' '.join(tok.tokenize(left_string))\n",
    "        right_new = ' '.join(tok.tokenize(right_string))\n",
    "        \n",
    "        outfile.write(left_new)\n",
    "        outfile.write('\\t')\n",
    "        outfile.write(right_new)\n",
    "        outfile.write('\\t')\n",
    "        outfile.write(conv_id)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "cornell_valid_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_cnt = Counter()\n",
    "\n",
    "with open('/data/users/kyle.shaffer/dialog_data/cornell_movie_dialog_no_context_valid_retok.txt', mode='r') as infile:\n",
    "    for line in infile:\n",
    "        left, right, _ = line.strip().split('\\t')\n",
    "        vocab_cnt.update(left.split())\n",
    "        vocab_cnt.update(right.split())\n",
    "        \n",
    "print('Done with valid set...')\n",
    "        \n",
    "with open('/data/users/kyle.shaffer/dialog_data/cornell_movie_dialog_no_context_train_retok.txt', mode='r') as infile:\n",
    "    for line in infile:\n",
    "        left, right, _ = line.strip().split('\\t')\n",
    "        vocab_cnt.update(left.split())\n",
    "        vocab_cnt.update(right.split())\n",
    "        \n",
    "print('Done with training set...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vocab_df = pd.DataFrame({'word': list(vocab_cnt.keys()), 'w_count': list(vocab_cnt.values())})\n",
    "print(movie_vocab_df.shape)\n",
    "movie_vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vocab_df[movie_vocab_df.w_count >= 10].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vocab_df[movie_vocab_df.w_count >= 6].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(movie_vocab_df[movie_vocab_df.w_count >= 6].word.apply(lambda x: x.lower()).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vocab_df[movie_vocab_df.w_count >= 6].w_count.sum() / movie_vocab_df.w_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skips = {'<SOD>', '<EOD>'}\n",
    "non_overlap_words = [w for w in movie_vocab_df[movie_vocab_df.w_count >= 6].word.tolist() \\\n",
    "                     if (w not in set(polar_vocab.word.values)) and not(w in skips)]\n",
    "print(len(non_overlap_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vocab_df['word_lower'] = movie_vocab_df.word.apply(lambda x: x.lower())\n",
    "add_word_df = movie_vocab_df[movie_vocab_df.word_lower.isin(non_overlap_words)]\n",
    "add_word_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_word_df = add_word_df.drop_duplicates('word_lower')\n",
    "add_word_df = add_word_df[add_word_df.word_lower.notnull()]\n",
    "add_word_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading polar vocab and assigning ID's\n",
    "def get_vocab(vocab_file, min_freq:int=3):\n",
    "    special_toks = ['<UNK>', '<PAD>', '<s>', '</s>']\n",
    "    c = {}\n",
    "    word_idx = 1\n",
    "    # with tf.gfile.GFile(vocab_file, 'r') as infile:\n",
    "    with open(vocab_file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            w, count = line.strip().split('\\t')\n",
    "            count = int(count)\n",
    "            if (w in special_toks) or (count < min_freq):\n",
    "                continue\n",
    "            c[w.strip()] = word_idx\n",
    "            word_idx += 1\n",
    "        for st in special_toks:\n",
    "            if st not in c.keys():\n",
    "                if st == '<PAD>':\n",
    "                    c[st] = 0\n",
    "                else:\n",
    "                    c[st] = max(c.values()) + 1\n",
    "    print('VOCAB SIZE = {}'.format(len(c)))\n",
    "    return c\n",
    "\n",
    "polar_vocab = get_vocab(polar_vocab_path, min_freq=10)\n",
    "print(max(polar_vocab.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_add_id = max(polar_vocab.values()) + 1\n",
    "print(start_add_id)\n",
    "\n",
    "for add_word in add_word_df.word_lower.tolist():\n",
    "    polar_vocab[add_word] = start_add_id\n",
    "    start_add_id += 1\n",
    "    \n",
    "print('New vocab size:', len(polar_vocab))\n",
    "print('Max word ID:', max(polar_vocab.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/users/kyle.shaffer/dialog_data/polar_movie_combined_vocab.txt', mode='w') as outfile:\n",
    "    for k, v in polar_vocab.items():\n",
    "        outfile.write(k)\n",
    "        outfile.write('\\t')\n",
    "        outfile.write(str(v))\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out Re-setting the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab('/data/users/kyle.shaffer/dialog_data/polar_movie_combined_vocab.txt', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "\n",
    "model = load_model('/data/users/kyle.shaffer/chat_models/lstm_polar_chatbot_epoch34_loss4.107.h5',\n",
    "                  custom_objects={'sparse_loss': lambda x, y: K.sparse_categorical_crossentropy(x, y, True)})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_embed = model.get_layer('embedding_1').get_weights()[0]\n",
    "print(orig_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = len(vocab) - orig_embed.shape[0]\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embed = np.random.normal(size=(diff, orig_embed.shape[1]))\n",
    "print(new_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embed = np.vstack((orig_embed, new_embed))\n",
    "print(combined_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_embed; del orig_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    print(l.name, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_logits, orig_bias = model.layers[-1].get_weights()\n",
    "print(orig_logits.shape)\n",
    "print(orig_bias.shape)\n",
    "\n",
    "logits_diff = len(vocab) - orig_logits.shape[1]\n",
    "logits_add = np.random.normal(size=(orig_logits.shape[0], logits_diff))\n",
    "print(logits_add.shape)\n",
    "bias_add = np.random.normal(size=(logits_diff))\n",
    "\n",
    "logits_combined = np.hstack((orig_logits, logits_add))\n",
    "bias_combined = np.hstack((orig_bias, bias_add))\n",
    "print(logits_combined.shape)\n",
    "print(bias_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Model\n",
    "from keras.layers import Embedding, Dense\n",
    "from keras.initializers import Constant\n",
    "\n",
    "new_embed_layer = Embedding(input_dim=combined_embed.shape[0], output_dim=combined_embed.shape[1], \n",
    "                           embeddings_initializer=Constant(combined_embed), mask_zero=True, trainable=True)\n",
    "\n",
    "encoder_embed = new_embed_layer(model.layers[1].output)\n",
    "encoder_embed = model.layers[3](encoder_embed)\n",
    "encoder_outputs, _, _ = model.layers[4](encoder_embed)\n",
    "encoder_outputs2, state_h, state_c = model.layers[6](encoder_outputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_embed = new_embed_layer(model.layers[0].output)\n",
    "decoder_embed = model.layers[5](decoder_embed)\n",
    "decoder_outputs = model.layers[7](decoder_embed, initial_state=encoder_states)\n",
    "\n",
    "# Reconstructing logits\n",
    "orig_logits, orig_bias = model.layers[-1].get_weights()\n",
    "print(orig_logits.shape)\n",
    "print(orig_bias.shape)\n",
    "\n",
    "logits_diff = len(vocab) - orig_logits.shape[1]\n",
    "logits_add = np.random.normal(size=(orig_logits.shape[0], logits_diff))\n",
    "print(logits_add.shape)\n",
    "bias_add = np.random.normal(size=(logits_diff))\n",
    "\n",
    "logits_combined = np.hstack((orig_logits, logits_add))\n",
    "bias_combined = np.hstack((orig_bias, bias_add))\n",
    "print(logits_combined.shape)\n",
    "print(bias_combined.shape)\n",
    "\n",
    "logits = Dense(units=len(vocab), activation='linear', name='logits')\n",
    "logits_out = logits(decoder_outputs)\n",
    "\n",
    "model = Model(inputs=[model.layers[1].output, model.layers[0].output], outputs=logits_out)\n",
    "model.get_layer('logits').set_weights([logits_combined, bias_combined])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
